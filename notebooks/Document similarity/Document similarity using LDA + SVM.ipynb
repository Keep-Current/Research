{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the twitter 'hot-list' papers as a training-set, build a classifier to detect which papers will be the next hot thing.\n",
    "\n",
    "1. divide every paper into paragraphs.\n",
    "2. run LDA/LSA on each paragraph, to get a topic list and compute topic similarity with new documents, or:\n",
    "3. user LDA2VEC on the documents and calculate the vector distance to new documents.\n",
    "4. Train SVM on the topics/vectors as features & binary/score output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import nltk\n",
    "import re\n",
    "import urllib.request\n",
    "import ujson\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim.corpora import Dictionary, HashDictionary, MmCorpus\n",
    "from gensim import models, utils, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/liad/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/liad/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIAL_CHARS = '[^A-Za-z0-9 ]+'\n",
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(text):\n",
    "    tokens = [re.sub(SPECIAL_CHARS, '', word.lower()) for word in nltk.word_tokenize(text)]\n",
    "    tokens = [wordnet_lemmatizer.lemmatize(word) for word in tokens if word not in stopwords]\n",
    "    tokens = list(filter(None, tokens))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus(object):\n",
    "    def __iter__(self):\n",
    "        for file in glob.glob(\"data/*.txt\"):\n",
    "            print(file)\n",
    "            paper = Path(file).read_text(encoding='utf8')\n",
    "            yield paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
